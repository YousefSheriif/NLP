{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-03-17T09:44:58.970651Z",
          "start_time": "2024-03-17T09:44:58.917309Z"
        },
        "id": "QhAlx-BozcD1"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "Lemmatizer = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-03-17T09:44:39.603243Z",
          "start_time": "2024-03-17T09:44:39.582841Z"
        },
        "id": "aIUaWAXnCfnF"
      },
      "outputs": [],
      "source": [
        "def Extract_Text(URL):\n",
        "    Response = requests.get(URL)\n",
        "    Accessed_Text = BeautifulSoup(Response.text, 'html.parser')\n",
        "    Text = ' '.join([p.get_text() for p in Accessed_Text.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6'])])\n",
        "    return Text\n",
        "\n",
        "def Clean_Text(Text):\n",
        "    Cleaned_Text = re.sub(r'[^\\w\\s]', '', Text)  # Remove punctuation\n",
        "    Cleaned_Text = re.sub(r'\\d+', '', Cleaned_Text)  # Remove digits\n",
        "    Cleaned_Text = Cleaned_Text.lower()\n",
        "    return Cleaned_Text\n",
        "\n",
        "def Tokenize_Text(Text):\n",
        "    tokens = word_tokenize(Text)\n",
        "    return tokens\n",
        "\n",
        "def Lemmatize_Text(tokens):\n",
        "    Lemmatized_Tokens = []\n",
        "    for token in tokens:\n",
        "        if Lemmatizer(token)[0].lemma_ != '-PRON-':\n",
        "            Lemmatized_Token = Lemmatizer(token)[0].lemma_\n",
        "        else:\n",
        "            Lemmatized_Token = token\n",
        "        Lemmatized_Tokens.append(Lemmatized_Token)\n",
        "    return Lemmatized_Tokens\n",
        "\n",
        "\n",
        "def Remove_Stopwords(tokens):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "    return filtered_tokens\n",
        "\n",
        "\n",
        "def Get_Unique_Words(text):\n",
        "    Unique_Words = set(text)\n",
        "    Sorted_Words = sorted(Unique_Words)\n",
        "    return '\\n'.join(Sorted_Words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-03-17T09:44:43.291747Z",
          "start_time": "2024-03-17T09:44:41.630462Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeRyQDghJDR1",
        "outputId": "d0068385-b689-497d-c732-f6611187d2cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ability\n",
            "accurately\n",
            "achieve\n",
            "acl\n",
            "acquire\n",
            "action\n",
            "actr\n",
            "address\n",
            "advance\n",
            "advantage\n",
            "age\n",
            "aid\n",
            "alan\n",
            "algorithm\n",
            "alignment\n",
            "along\n",
            "already\n",
            "alsoedit\n",
            "although\n",
            "among\n",
            "analysisedit\n",
            "analyze\n",
            "announce\n",
            "answer\n",
            "anymore\n",
            "apparent\n",
            "application\n",
            "applicationsedit\n",
            "apply\n",
            "approach\n",
            "approachedit\n",
            "area\n",
            "article\n",
            "articulate\n",
            "artificial\n",
            "aspect\n",
            "automate\n",
            "base\n",
            "become\n",
            "behaviour\n",
            "bengio\n",
            "beyond\n",
            "branch\n",
            "british\n",
            "brno\n",
            "broadly\n",
            "build\n",
            "call\n",
            "capable\n",
            "capture\n",
            "care\n",
            "categorize\n",
            "category\n",
            "cause\n",
            "challenge\n",
            "chinese\n",
            "chomskyan\n",
            "closely\n",
            "cluster\n",
            "coarse\n",
            "coauthor\n",
            "cognition\n",
            "cognitionedit\n",
            "cognitive\n",
            "collection\n",
            "college\n",
            "combine\n",
            "common\n",
            "commonly\n",
            "complex\n",
            "comprehension\n",
            "computational\n",
            "compute\n",
            "computer\n",
            "concern\n",
            "conference\n",
            "confront\n",
            "conll\n",
            "construction\n",
            "contain\n",
            "content\n",
            "context\n",
            "contextedit\n",
            "contextual\n",
            "convenience\n",
            "corpora\n",
            "corpus\n",
            "couple\n",
            "cpu\n",
            "criterion\n",
            "dataset\n",
            "datum\n",
            "decision\n",
            "deep\n",
            "define\n",
            "dependency\n",
            "develop\n",
            "development\n",
            "developmental\n",
            "devise\n",
            "dictionary\n",
            "direct\n",
            "direction\n",
            "directionsedit\n",
            "discourage\n",
            "discourse\n",
            "division\n",
            "document\n",
            "dominance\n",
            "drawback\n",
            "due\n",
            "early\n",
            "either\n",
            "elaborate\n",
            "electronic\n",
            "embedding\n",
            "emulate\n",
            "end\n",
            "energy\n",
            "engineering\n",
            "especially\n",
            "example\n",
            "experience\n",
            "experiment\n",
            "explainability\n",
            "explicit\n",
            "external\n",
            "extract\n",
            "extrapolate\n",
            "far\n",
            "feature\n",
            "field\n",
            "finding\n",
            "first\n",
            "flurry\n",
            "follow\n",
            "framework\n",
            "free\n",
            "frequently\n",
            "friston\n",
            "functional\n",
            "future\n",
            "general\n",
            "generation\n",
            "george\n",
            "give\n",
            "goal\n",
            "good\n",
            "gradual\n",
            "grammar\n",
            "hand\n",
            "handcode\n",
            "handwritten\n",
            "hard\n",
            "health\n",
            "healthcare\n",
            "help\n",
            "heritage\n",
            "heuristic\n",
            "hide\n",
            "higherlevel\n",
            "historical\n",
            "historically\n",
            "historyedit\n",
            "however\n",
            "human\n",
            "idea\n",
            "ifthen\n",
            "important\n",
            "improve\n",
            "inaccessible\n",
            "include\n",
            "increase\n",
            "increasingly\n",
            "individual\n",
            "inefficiency\n",
            "information\n",
            "inherent\n",
            "insight\n",
            "intelligence\n",
            "intelligent\n",
            "interdisciplinary\n",
            "intermediate\n",
            "interpretation\n",
            "intertwine\n",
            "introduction\n",
            "involve\n",
            "john\n",
            "karl\n",
            "knowledge\n",
            "lakoff\n",
            "language\n",
            "large\n",
            "late\n",
            "law\n",
            "layer\n",
            "learn\n",
            "length\n",
            "less\n",
            "lessen\n",
            "lexical\n",
            "likewise\n",
            "limit\n",
            "linguistic\n",
            "linksedit\n",
            "list\n",
            "llm\n",
            "london\n",
            "longstande\n",
            "lookup\n",
            "machine\n",
            "machinelearne\n",
            "machinery\n",
            "mainstream\n",
            "maintain\n",
            "major\n",
            "make\n",
            "manipulate\n",
            "many\n",
            "markov\n",
            "match\n",
            "measure\n",
            "medicine\n",
            "mental\n",
            "method\n",
            "methodology\n",
            "mid\n",
            "mikolov\n",
            "million\n",
            "mind\n",
            "model\n",
            "moore\n",
            "morphological\n",
            "mostly\n",
            "multilayer\n",
            "multimodal\n",
            "natural\n",
            "naturallanguage\n",
            "necessary\n",
            "need\n",
            "network\n",
            "networkbase\n",
            "networksedit\n",
            "networkstyle\n",
            "neural\n",
            "neuroscience\n",
            "neuroscientist\n",
            "nevertheless\n",
            "new\n",
            "ngram\n",
            "nlp\n",
            "note\n",
            "notion\n",
            "nuance\n",
            "observe\n",
            "obsolete\n",
            "offer\n",
            "old\n",
            "one\n",
            "operationalizable\n",
            "operationalization\n",
            "organize\n",
            "otherwise\n",
            "overperformed\n",
            "parse\n",
            "part\n",
            "particular\n",
            "partly\n",
            "partofspeech\n",
            "patient\n",
            "perceptron\n",
            "period\n",
            "perspective\n",
            "phd\n",
            "phrasebook\n",
            "popularity\n",
            "possible\n",
            "power\n",
            "premise\n",
            "presence\n",
            "presentedit\n",
            "previously\n",
            "primarily\n",
            "principle\n",
            "privacy\n",
            "probabilistic\n",
            "problem\n",
            "process\n",
            "processingedit\n",
            "produce\n",
            "property\n",
            "propose\n",
            "protect\n",
            "psycholinguistic\n",
            "psychology\n",
            "publish\n",
            "pursue\n",
            "question\n",
            "rarely\n",
            "readingedit\n",
            "realworld\n",
            "recently\n",
            "recognition\n",
            "record\n",
            "recurrent\n",
            "refer\n",
            "referencesedit\n",
            "relational\n",
            "replace\n",
            "represent\n",
            "representation\n",
            "require\n",
            "research\n",
            "result\n",
            "revive\n",
            "revolution\n",
            "room\n",
            "root\n",
            "rule\n",
            "rulebase\n",
            "science\n",
            "scientific\n",
            "searle\n",
            "sedit\n",
            "see\n",
            "seek\n",
            "semantic\n",
            "sense\n",
            "sentencesedit\n",
            "separate\n",
            "sequencetosequence\n",
            "series\n",
            "serve\n",
            "set\n",
            "several\n",
            "share\n",
            "show\n",
            "similar\n",
            "simple\n",
            "since\n",
            "single\n",
            "solve\n",
            "sort\n",
            "speak\n",
            "specifically\n",
            "speech\n",
            "ssedit\n",
            "start\n",
            "stateoftheart\n",
            "statistical\n",
            "steady\n",
            "stem\n",
            "step\n",
            "still\n",
            "strong\n",
            "student\n",
            "study\n",
            "subdivide\n",
            "subfield\n",
            "subtask\n",
            "support\n",
            "symbol\n",
            "symbolic\n",
            "syntactic\n",
            "system\n",
            "tag\n",
            "task\n",
            "tasksedit\n",
            "technical\n",
            "technically\n",
            "technique\n",
            "technology\n",
            "tendency\n",
            "test\n",
            "text\n",
            "thennewlyinvente\n",
            "theoretical\n",
            "theoretician\n",
            "theory\n",
            "think\n",
            "though\n",
            "three\n",
            "tie\n",
            "time\n",
            "title\n",
            "tomáš\n",
            "tool\n",
            "topic\n",
            "towards\n",
            "train\n",
            "trajectory\n",
            "transformation\n",
            "transformational\n",
            "translation\n",
            "tree\n",
            "trend\n",
            "ture\n",
            "turn\n",
            "two\n",
            "underlie\n",
            "underpinning\n",
            "understand\n",
            "university\n",
            "uptake\n",
            "use\n",
            "various\n",
            "well\n",
            "wellsummarize\n",
            "whose\n",
            "widespread\n",
            "winter\n",
            "within\n",
            "word\n",
            "wordvec\n",
            "would\n",
            "write\n",
            "year\n",
            "yoshua\n"
          ]
        }
      ],
      "source": [
        "URL = 'https://en.wikipedia.org/wiki/Natural_language_processing'\n",
        "Text = Extract_Text(URL)\n",
        "Cleaned_Text = Clean_Text(Text)\n",
        "Tokens = Tokenize_Text(Cleaned_Text)\n",
        "Lemmatized_Tokens = Lemmatize_Text(Tokens)\n",
        "Filtered_Tokens = Remove_Stopwords(Lemmatized_Tokens)\n",
        "Unique_Words = Get_Unique_Words(Filtered_Tokens)\n",
        "Unique_Words = Unique_Words.split('\\n')\n",
        "Filtered_Unique = [word for word in Unique_Words if len(word) > 2]\n",
        "Filtered_Text = '\\n'.join(Filtered_Unique)\n",
        "print(Filtered_Text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
